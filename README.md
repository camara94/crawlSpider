# crawlSpider
Scrapy nous donne accès à deux classes principales de spiders, le spiders générique que nous avons beaucoup utilisé auparavant dans d'autres vidéos et cette CrawlSpider qui fonctionne de manière légèrement différente. Nous pouvons lui donner un ensemble de règles et lui faire suivre les liens automatiquement, en passant ceux que nous voulons faire correspondre à notre fonction d'analyseur avec un callback. Cela rend incroyablement facile la suppression complète des données du site Web. Dans ce tuto, je vais vous expliquer comment utiliser le CrawlSpider, ce que font le Ruler et le LinkExtrator et comment les utiliser, ainsi qu'une démonstration de leur fonctionnement.
